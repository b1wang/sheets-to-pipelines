# Week 00 - Kickoff

Date: March 14th, 2025

## Purpose of Project

This project marks the start of my endeavor to convert an almost entirely Excel-based data ecosystem into a structured data system with automated pipelines and ML / AI deployments. The goal is to leverage this project as a practical learning environment using company data and gaining experience deploying ML models following an industry-standard approach. 

Constraints:
- Since these workflows should be in-house only and deal with company-sensitive data, all proprietary data should be securely stored in a local server (no cloud-based). 
- As this is also a learning project, should avoid bootstrapped or no-code solutions. 

## Initial Setup
1. Repository creation
    - Created sheets-to-pipelines repository.
    - Added an initial README.md and LICENSE (CC-BY-NC-ND 4.0 license).
2. Basic project structure and Docker setup
    - Created initial root project structure and documentation folders.
    - Created Docker containers for PostgreSQL, Juptyter Notebook, Airflow, and custom Python env for running scripts.
    - Mounted persistent storage using Docker volumes.
    - Set .env variables
3. Begin configuring Postgres database
    - Created products table and inserted current active skus.

## Next Steps

At its core, this is a data-driven application that an employee could use to derive insights from stored SQL data, make predictions using ML forecasting, interact and manage the PostgreSQL database, and finally generate reports for other teams and management.

The current, provisional tech stack: 
| Component   | Tech |
| --------    | ------- |
| Frontend    | Streamlit    |
| Backend API | FastAPI + Pydantic |
| Database    | PostgreSQL |
| Data Pipelines | Python (pandas, SQLAlchemy)
| Automation | Airflow (DAGs) | 
| ML | XGBoost, Scikit-learn, TensorFlow | 
| Deployment | Docker + CI/CD (GitHub Actions) |
| DevOps | GitHub Project (Kanban)

Since this project has a mix of software development, data engineering, and ML engineering, I believe following a hybrid Kanban + CRISP-DM approach will be best.
- Kanban for building FastAPI, Streamlit, ETL pipelines and managing incremental updates.
- CRISP-DM for defining and training ML models and ensuring structured model evaluation before deployment. 

Next week, I want to:
- Create the GitHub project and start tracking tasks.
- Refine the products schema to include historical tracking of prices
- Create the sales and inventory schemas
- Starting setting up Streamlit and FastAPI 

## Reflection 

My original background is in Software development, so my first thought was to follow an Agile scrum development process. Based on research, I found that scrum seems to perform poorly on data-driven applications, and it seemed like Kanban / CRISP-DM served as better lifecycles. However, as a solo developer, part of me was tempted to just throw my tasks into my Notion management system that I already use for my daily life, and integrate my project workflow through that.

In the spirit of expanding technologies and skills, however, I decided to get familiar with GitHub projects and issues. 










